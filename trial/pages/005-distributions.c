/* Schwartz space, Fourier transform is automorphism on this space:  one can define a perfect duality for tempered distributions. This is characterized by all derivatives rapidly decreasing, which is a strange property to have.  */
/* Ring of differential operators, Sobolev space:  take single index (so it is derivatives to a single variable) ring of linear differential operators this is simply a linear combination of finitely many partial derivatives of regularity parameter m, take the root of m, and you get the Hilbert norm or L^p norm or the Sobolev norm. Is there a cheap connection between algebra and analysis here? */
/* Hilbert norm, Sobolev spaces:  one study L^p spaces because one wants to make a L into some sort of variable term in a polynomial. From considerations of Euclidean geometry, it seems very natural to consider the sum of p powers normalised by a root. You can pick L to be anything you wish, in this case L is an operator, and then study norms on polynomial like objects on it for some striking objects to study. If I am not mistaken, the main idea behind Sobolev spaces are when you pick the operator L as the differential operator D. */
/* Distributions, example of the methodology of adopting weak solutions:  one of the most interesting things you can do once you have no possible conception of a solution to a problem is to weaken the notion of a solution. With the right weakening, one can still derive insight and define objects worth studying from a situation where there is no true solution to the problem. This is the approach of many a business analyst, and in the real world where there is too much complexity. */
/* Test functions, imagine a bell curve that is not only smooth (infinitely differentiable) but also "dies off" to exactly zero outside a finite region. It's a "blip" or a "bump." For example, the function p(x)=exp(−1/1-x^2)​ for norm(x) < 1  and 0 otherwise. It's perfectly smooth everywhere, even at the boundary of its support, where it flattens out to zero. These functions are ideal "probes" to "test" the properties of more general objects: A test function is a smooth function with compact support. The space of test functions, denoted D(Rn), consists of all infinitely differentiable functions p: R^n→C such that the closure of the set {x in R^n | p(x) is nonzero} is compact. */
/* Distribution, distribution as a "machine" that takes in a test function "blip" and spits out a number. The output number is the "average" of the distribution "measured" by the test function. For a regular function f(x), the distribution it defines is the integral ∫ f(x) p(x) dx. The famous Dirac delta function d is a distribution, not a regular function. It takes a test function p(x) and outputs its value at the origin: d(p) = p(0). Visually, it's a "probe" that measures a function at a single point: A distribution (or generalized function) is a continuous linear functional on the space of test functions. This means it's a linear map T:D(R^n) to C that is continuous with respect to the specific topology of D(R^n). */
/* Distribution, a "rough" function, like a step function. Convolving it with a small, smooth bump function ρ_eps ​(x) is like "blending" or "smoothing" the sharp edges. It produces a perfectly smooth function that is very close to the original rough one. This process, called regularization, is critical in proving results in PDEs where a smooth approximation is needed: A mollifier is a special kind of test function, a "bump function" often denoted ρ_esp (x), with integral 1 and support shrinking to a point as esp →0. We can use convolution with a mollifier to approximate any function with a smooth one. */
/* Weak derivatives, take the derivative of "functions" that are not differentiable in the classical sense. For example, the Heaviside step function H(x), which is 0 for x<0 and 1 for x>0, is not differentiable at x=0. Its distributional derivative is the Dirac delta function d(x), a spike at the origin. This makes intuitive sense: the "change" in the Heaviside function is an infinite jump at a single point:  The derivative of a distribution T is defined via integration by parts. For a test function d, the distributional derivative T′is defined as T′(d)=−T(d′). This formula holds for all test functions and can be extended to all distributions. */
/* Test functions, the Schwartz space as including functions like exp(-x^2). They are infinitely smooth and "die off" to zero very quickly, but their support is the entire real line, so they aren't test functions. The space of "tempered distributions" is the dual of the Schwartz space, a much broader class of distributions that includes all regular functions that don't grow too fast, as well as the Dirac delta function:  The Schwartz space S(R^n) is a larger space than D(R^n). It consists of all smooth functions that, along with all their derivatives, decay faster than any polynomial. Test functions are a subset of the Schwartz space. */
/* Distributions, Fourier transform is a powerful tool for analyzing frequency content. By extending it to distributions, we can find the frequency content of things like the Dirac delta function. The Fourier transform of d(x) is the constant function 1. This makes intuitive sense: a spike at one point contains all frequencies equally:  The Fourier transform can be extended to tempered distributions. For a tempered distribution T, its Fourier transform T^ is defined by T^(d) =T(d^​), where d is a test function in the Schwartz space. */
/* Test functions, topology, a sequence of "bumps" that are getting closer and closer to another "bump." For them to converge in this space, they can't just converge in value; their derivatives must also converge, and they must all stay within a single, bounded region. This ensures that the distributions defined on this space are well-behaved:  The space of test functions D has a special topology that makes it a locally convex topological vector space. A sequence of test functions d_n​ converges to d if all their derivatives converge uniformly and their supports are contained in a common compact set. */
/* Test function, fundamental solution is like the "response" of a system to a single, concentrated "poke" (the Dirac delta). The log function in R^2 blows up at the origin, just like the response to a single-point heat source or charge. This is a very powerful concept for solving PDEs, as the solution for any given right-hand side can be found by convolving with the fundamental solution:  The fundamental solution of a linear PDE is a distribution E that solves the equation when the right-hand side is the Dirac delta function. For the Laplacian, the fundamental solution in R^2 is E(x)= 1/2 pi log(|x|). */
/* Test functions, manifold, like a sphere. We can cover it with a finite number of overlapping "patches." We can create a set of test functions, one for each patch, that are 1 in the middle of their patch and smoothly go to 0 at the boundary. These functions can be used to break down a global problem into local problems. The sum of all these functions is exactly 1 everywhere: A smooth partition of unity is a collection of smooth functions with compact support (test functions) that sum to 1 over a manifold. */
/* Test functions, distributions as an algebraic completion of the space of continuous functions. Just as we add "ideal numbers" (like the root of -1​) to the real numbers to get the complex numbers, we add "ideal functions" (like the Dirac delta) to the space of functions to get the space of distributions:  Distributions provide a way to work with "ideal" objects that are not functions, such as the Dirac delta function. They are abstract, but their properties are well-defined. */
/* Distribution, a regular function f, the distribution is the integral, integral of f(x) p(x)dx. This is like a weighted average of f(x) with the test function p(x) as the weight. Distributions allow us to do this "averaging" even for objects that aren't functions: A distribution is a way to generalize the process of integration. Instead of integrating a function against a measure, a distribution "integrates" a test function. */
/* Distribution, as "points" in a very abstract, infinite-dimensional space. The geometric Hahn-Banach theorem says we can find a hyperplane (defined by a test function) that separates two disjoint convex sets of distributions:  The space of distributions can be viewed through the lens of functional analysis. The dual of the space of test functions is the space of distributions. The geometric Hahn-Banach theorem can be applied to separate convex sets of distributions. */
/* Test functions, a membrane stretched over a boundary. We can apply a point load (a Dirac delta function) at a specific point. The solution to this problem is a distribution:  Distributions can be used to formulate and solve boundary value problems in a weak sense. */
/* Distribution, probability of a random variable taking on a specific value in a continuous distribution is zero. However, we can describe this event using the Dirac delta distribution, which represents the probability mass at a single point: A probability density function is a distribution. The Dirac delta function can represent a discrete probability measure at a point. */
/* Distribution, graph of an operator is the set of all input-output pairs. The closed graph theorem states that for a well-behaved space like distributions, if the graph of an operator is a "complete" set (it doesn't have any holes), then the operator must be continuous:  The closed graph theorem is a fundamental result in functional analysis. It can be used to show that a linear operator on the space of distributions is continuous. */
/* Distribution kernel, consider the identity operator on a space of functions. Its kernel is the Dirac delta distribution, K(x,y)=δ(x−y).This functional "picks out" the value of the input function at the point x. Geometrically, imagine the kernel as a "point-source" or "spike" at every point on the diagonal line x=y in the (x,y)-plane. When you integrate (or more accurately,"convolute") with an input function, the spike at each point (x,x) instantaneously transfers the value of the input function at x to the output function at x. The action of the operator is to leave the function unchanged: A distribution kernel K(x,y) is an element of a space of distributions that defines a linear operator T mapping functions from one space to another, as given by the integral of K(x, y) * phi(y) dy in a general sense for phi(y) test function. */
/* Distribution kernel, manifold (a curved space), solving a differential equation for a point source gives a Green's function. This function, which is a distribution kernel, tells you how a disturbance at one point propagates to all other points. For example, on a sphere, the Green's function for the Laplacian is a kernel that describes the potential from a point charge: A distribution kernel is a Green's function on a manifold. */
/* Distribution kernel, imagine a space where each point has both a position and a momentum (like a particle). A distribution kernel in this context isn't just a function of position but also of momentum. It acts on a function by transforming it not just locally but also in terms of its oscillatory behavior. This allows it to model phenomena like wave propagation, where both position and frequency are critical: A distribution kernel is a pseudodifferential operator on the cotangent bundle. */
/* Distribution kernel, space of test functions, say smooth functions with compact support, is a kind of vector space. A distribution kernel is a "generalized vector" in the dual space. It takes a function as input and outputs a number, like a dot product. The Dirac delta kernel δ(f)=f(0) is a simple example: it takes a function and "selects" its value at the origin: A distribution kernel is a continuous linear functional on a space of test functions. */
/* Distribution kernel, a random variable can be discrete, continuous, or a mix of both. A distribution kernel can represent the probability density for all three cases. For a discrete variable, the density is a sum of Dirac delta functions, one for each possible value, with a weight equal to its probability. For a continuous variable, the density is a smooth function. The kernel provides a unified framework to handle both: A distribution kernel is a probability density function, but extended to include impulses. */
/* Distribution kernel, "mass distribution" on a line. This mass isn't necessarily concentrated at a point but can be smeared out. A distribution kernel is the mathematical object that describes this mass. When a smooth function (the test function) is "dropped" onto this mass distribution, the output is the total weighted mass. A Dirac delta is a point mass. Its action on a test function is simply to "weigh" the function at the location of the point mass: A distribution kernel is a signed measure that acts on continuous functions with compact support. */
/* Distribution kernel, simple operator like the Laplacian on a circle, its "eigenvectors" are the sines and cosines. For a more general operator, the "eigenvectors" are not necessarily functions but distributions. The distribution kernel represents these "eigen-distributions," which are the generalized functions that are only scaled by the operator. For example, the Dirac delta function δ is an "eigen-distribution" of the convolution operator with itself, because δ∗δ=δ: A distribution kernel is the "eigen-distribution" of an operator. */
/* Distribution kernel, convex function like f(x)=|x|, the subgradient at x=0 isn't a single number but a set of all slopes between −1 and 1. A distribution kernel can represent this set as a generalized function. Imagine the graph of f(x)= |x| as a V-shape. At the sharp point (the origin), the "tangent line" isn't unique. The subgradient is the set of all possible slopes of lines that lie below the graph and pass through the origin. A distribution kernel is a formal way to describe this "set of slopes.": A distribution kernel represents a subgradient of a convex function. */
/* Distribution kernel, a stochastic process like a random walk, the transition density tells you the probability of a particle moving from one location to another. This density is a distribution kernel. For example, for a standard Brownian motion, the transition density is a Gaussian function centered at the starting point, whose variance increases with time: A distribution kernel is the transition density of a stochastic process. */
/* Distribution kernel, Gâteaux derivative is the "directional derivative" of an operator in an infinite-dimensional space. It tells you how the operator changes as you move a small distance in a specific direction. The Gâteaux derivative is often a linear operator, and its kernel is the distribution kernel: A distribution kernel is a Gâteaux derivative of a nonlinear operator. */
/* Distribution kernel, semigroup is a family of operators that describes evolution over time, like the heat semigroup. The generator is the "infinitesimal" operator that tells you how the system evolves from one moment to the next. For the heat equation, the generator is the Laplacian operator, and its kernel is the fundamental solution: A distribution kernel is the generator of a semigroup of operators. */
/* Distribution kernel, character is a special type of function that helps to decompose a representation of a group into simpler pieces. The character is a distribution kernel because it acts as a "probe" that measures how a function on the group is transformed by a group action. For example, the characters of the group of rotations in the plane are the functions exp(inθ), which are kernels that pick out the n-th harmonic of a function: A distribution kernel is a character of a locally compact group. */
/* Distribution, convex function like f(x)= |x|, the subgradient at x=0 is a set of all possible slopes between −1 and 1. The Dirac delta distribution can represent this subgradient. It's a way of representing a "sharp change" or "singularity" in the derivative as a well-defined object: A distribution can be the subgradient of a convex function at a point of non-differentiability. */
/* Distribution, operator like the derivative operator, its eigenfunctions are not always functions in the classical sense. The eigenfunctions of the momentum operator are exp(iξx), which are not in any L2 space. These are generalized eigenfunctions (distributions). The operator "acts" on these distributions by simply scaling them, revealing its spectrum: A distribution is a generalized eigenfunction of an operator. */
/* Distribution, space of test functions is a vector space of "nice" functions. A distribution is a linear map from this space to the real or complex numbers. The "space of all distributions" is the dual space of the space of test functions: A distribution is a continuous linear functional on the space of smooth functions with compact support, C_c^\infty. */
/* Distribution, manifold (a curved space), the concept of a function is generalized to a section of a vector bundle. A distribution is a generalized section. For example, the Dirac delta function on a sphere is a distribution on the sphere that can be defined intrinsically: A distribution is a section of a vector bundle over a manifold. */
/* Distribution, stochastic process like a random walk can be described by its distribution. The position of the walker is a random variable. The distribution of this random variable is a generalized function that can have point masses: A distribution is a generalized random variable. */
/* Distribution, heat equation describes the diffusion of heat. The initial temperature distribution can be a point source, which is a Dirac delta distribution. The semigroup of heat operators takes this initial distribution and produces a smooth function that describes how the heat spreads out over time: A distribution can be the initial state of a system described by a semigroup. */
/* Distribution, Fourier transform of a distribution (e.g., a singular object like the Dirac delta) can be a well-behaved function (e.g., a constant function). The distribution is an object that, when viewed in the frequency domain, becomes a simple function: A distribution is an object whose Fourier transform is a function. */
/* Distribution, impulse response is the output of a system when the input is a Dirac delta function (a perfect impulse). This response, which is a distribution, completely characterizes the system's behavior: A distribution is the impulse response of a linear time-invariant system. *//* Modulated rescale bump function, standard "bump" on a flat surface, centered at the origin, representing the function ψ(x). The rescaling by λ acts like a zoom lens. If λ is large, the bump becomes very narrow and tall, localized in position but spread out in frequency. If λ is small, the bump becomes wide and flat, spread out in position but localized in frequency. The modulation by exp(iξx()acts as a "wave" that rides on top of the bump. The function ψ_{λ,ξ}​(x) is a wave of frequency ξ that is localized within a region of size 1/λ and centered at the origin. We can use this family of functions to test the regularity of a distribution by "probing" it at different scales and frequencies. The Sobolev norm of a distribution can be characterized by how well it can be "seen" by these bump functions:  In distribution theory, a modulated rescaled bump function is a smooth function with compact support (a "bump function," ψ) that is transformed by a change of scale and a modulation. Specifically, it is a function of the form ψ_{λ,ξ}​(x)=exp(iξ⋅x)ψ(λx)), where λ>0 is a scaling parameter and ξ in R^n is a modulation parameter. This family of functions is crucial because it forms a "wavelet-like" basis that can probe the regularity of a distribution at a specific point in both position (x) and frequency (ξ) space. */
/* Modulated rescaled bump function, imagine a "phase space" where the horizontal axis represents position and the vertical axis represents frequency. The wave front set of a distribution is a cloud of points in this space that tells you where and in what "frequency" (or direction) the distribution is "not smooth." A modulated rescaled bump function is a small "probe" that we can move around in this phase space. The function ψλ,ξ​(x) is a probe that is localized at a certain position and a certain frequency. By seeing how the distribution "responds" to this probe, we can determine if a point (x0​,ξ0​) is in the wave front set. If the distribution "blows up" when we test it with a probe at (x0​,ξ0​), then that point is in the wave front set. If the response is "tame," then the distribution is smooth at that point and in that direction:  In microlocal analysis, the modulated rescaled bump function is a fundamental tool for analyzing the wave front set of a distribution. The wave front set is a set of points in the phase space (T∗R^n is isomorphic R^{2n}), which captures the location and direction of the singularities of a distribution. The function ψ_{λ,ξ}​(x) is used to test the behavior of a distribution near a point (x0​,ξ0​) in phase space. The scaling parameter λ controls the size of the neighborhood in position and frequency space, while the modulation ξ controls the direction of the frequency. */
/* Modulated rescaled bump function, a distribution as a complex signal. We want to analyze its components at different frequencies. A modulated rescaled bump function is a "filter" that isolates a specific frequency band at a specific location. The operator that maps a distribution u to the family of coefficients {⟨u,ψλ,ξ​⟩}λ,ξ​ is like a time-frequency analyzer. This analyzer tells us how much "energy" the signal has at a particular location and frequency. A regular distribution would have very little "energy" at high frequencies, which means that the coefficients ⟨u,ψ_{λ,ξ}​⟩ would decay quickly as ξ becomes large. The Sobolev norm is essentially a weighted sum of these coefficients, and the decay rate of the coefficients tells us about the regularity of the distribution:  From a functional analysis viewpoint, the modulated rescaled bump function can be seen as an element of a function space, and its use in distribution theory can be formalized as the action of a probing operator. For a distribution u, the value ⟨u,ψ_{λ,ξ}​⟩ is a coefficient that measures the "content" of the distribution at a certain scale and frequency. This family of coefficients, as λ and ξ vary, forms a kind of "fingerprint" of the distribution's regularity. */
/* Modulated rescaled bump function, piece of music with a bass drum beat and a high-pitched flute melody. The Fourier transform gives us the overall frequency content but doesn't tell us when each sound occurs. The STFT, using a window function, allows us to analyze the frequency content of the music in short time intervals. The modulated rescaled bump function is the perfect window for this. It is a "time-limited" sinusoid. We slide this window along the signal, and for each position, we calculate the Fourier transform of the windowed signal. This gives us a spectrogram, which is a 2D plot of time versus frequency, and the brightness of the plot tells us the energy of the signal at that time and frequency. A smooth signal would have a spectrogram with "low-frequency" content, while a jagged signal would have a spectrogram with "high-frequency" content:  In signal processing, a modulated rescaled bump function is a "windowed sinusoid," which is used to perform a short-time Fourier transform (STFT). The STFT is a time-frequency analysis technique that is used to analyze non-stationary signals. The bump function acts as a window that localizes the signal in time, and the modulation provides the frequency component. */
/* Modulated rescaled bump function, shock wave propagating through a medium. The shock wave is a discontinuity in the pressure or density. It is a singularity in the solution to the PDE. The modulated rescaled bump function can be used to "interrogate" this singularity. We can use a bump function with a high frequency (ξ) that is localized near the shock wave to determine its exact location. The direction of the modulation vector ξ would align with the direction of the shock wave's normal vector. The regularity of the solution is then the property that it is smooth everywhere except for a set of points that are "detected" by these probes:  From the perspective of PDEs and hyperbolic equations, the modulated rescaled bump function is used to study how singularities (e.g., shock waves) propagate. The regularity of a solution is often a function of both position and direction. A singularity is a point where the solution is not smooth, and the modulated rescaled bump function can be used to probe this singularity to determine its location and direction of propagation. */
/* Modulated rescaled bump function, curved surface, like a sphere. We can't use a standard bump function that is defined on R^n directly. Instead, we use a local chart to "flatten" a small part of the sphere. On this flat part, we can use a standard modulated rescaled bump function. This allows us to "probe" the regularity of a function on the sphere at a specific point and in a specific direction. The regularity of the function on the manifold is its property of being smooth in every local chart, and the Sobolev norm on the manifold is a combination of the Sobolev norms of the function in each local chart:  In global analysis, we study objects on manifolds. A modulated rescaled bump function on a manifold is a smooth function with compact support that is modulated by a function that is "close" to a linear function in a local chart. The family of these functions forms a basis for a manifold, which can be used to study the regularity of objects (e.g., sections of bundles, solutions to PDEs) on the manifold. */
/* Modulated rescaled bump function, random landscape, where the height at each point is a random variable. The covariance function of the random field tells us how the height at one point is correlated with the height at another point. If the covariance function is smooth, the landscape is likely to be smooth. A modulated rescaled bump function can be used to probe the regularity of the landscape. We can use a bump function with a high frequency (ξ) that is localized at a point to "test" for roughness. If the variance of the integral of the random field against this probe is small, then the field is likely to be smooth at that point and in that direction:  In probability theory, the modulated rescaled bump function can be used to study the regularity of random fields or stochastic processes. For example, in the study of a Gaussian process, the covariance function determines the "smoothness" of the sample paths. The modulated rescaled bump function can be used to probe the regularity of the sample paths at a specific point in both position and frequency. */
/* Modulated rescaled bump function, imagine the Fourier transform of the picture gives us the frequency content, but it doesn't tell us where the features are. A wavelet transform gives us a representation of the picture in both space and frequency. A modulated rescaled bump function is like a tiny "pixel" that we can use to analyze the picture. The scale parameter λ controls the size of the pixel, and the modulation parameter ξ controls the "texture" that the pixel is looking for. The regularity of the picture is related to how much "energy" is in the high-frequency pixels. A smooth picture would have very little energy in the small, high-frequency pixels:  The modulated rescaled bump function is a close relative of a wavelet. A wavelet is a function that is localized in both time and frequency, and a family of wavelets can be used to represent any function in a function space. The modulated rescaled bump function is a special type of "wavelet" that is particularly useful for analyzing the regularity of functions and distributions. */
/* Modulated rescaled bump function, the a nonlinear PDE as a complex machine that takes a function and outputs another function. The solution to the PDE is a fixed point of this machine. We want to know how "smooth" the solution is. We can use a modulated rescaled bump function to "test" the solution for smoothness. The regularity of the solution is the property that it behaves "nicely" when we test it with these bump functions. The Sobolev embedding theorem, for example, says that if the solution is in a certain Sobolev space, then it must be continuous, which is a type of regularity that can be probed with these bump functions:  In nonlinear functional analysis, the modulated rescaled bump function can be used to study the regularity of solutions to nonlinear PDEs. The solutions to these equations are often in Sobolev spaces, and the regularity of the solution is a key property. The modulated rescaled bump function can be used to probe the regularity of the solution in a very localized way. */
/* Modulated rescaled bump function, an operator as a "vibrating string." The eigenvalues of the operator are the fundamental frequencies of the string. The eigenfunctions are the shapes of the vibrations. The modulated rescaled bump function can be used to create a "wave packet" that is localized in space and frequency. We can "probe" the operator with this wave packet to study how it affects functions with a specific regularity. The regularity of the eigenfunctions is a key property that can be studied using these wave packets:  In spectral theory, the modulated rescaled bump function can be used to study the spectral properties of an operator. The spectrum of an operator is the set of its eigenvalues, which are related to the operator's properties. The modulated rescaled bump function can be used to construct a "wave packet" that is localized in both position and frequency. This wave packet can be used to study the operator's action on functions with a specific regularity. */
/* Modulated rescaled bump function, two-dimensional plot with position on the x-axis and frequency on the y-axis. The modulated rescaled bump function is a "blob" in this space. The Heisenberg uncertainty principle says that the area of this blob is always greater than or equal to a certain constant. If we try to make the blob very narrow in position (by making λ large), it becomes very wide in frequency. If we try to make it very narrow in frequency (by making λ small), it becomes very wide in position. The regularity of a function is its property of having a "small" blob in this phase space:  The modulated rescaled bump function is a concrete example of a function that demonstrates the Heisenberg uncertainty principle in signal processing. The uncertainty principle states that it is impossible to perfectly localize a function in both position and frequency. The bump function is a compromise: it is localized in both, but its localization is not perfect */
