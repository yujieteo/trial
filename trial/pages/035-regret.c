/* Regret, suboptimality gap, comparison with best possible outcome with chosen arms: immediate definition is the expectation of the integral of the difference (suboptimality gap) between the best possible choice (argmax gives choice) and the learner's actual choice (argument). */
/* Sublinear regret, relative standard for regret: you will always regret, so you just want the algorithm such that the average regret goes to zero. */
/* Lai-Robbins lower bound, optimal performance constrained by information theory: the idea is to compare the suboptimal gap to the KL divergence between the probability distribution of arms. With information theory, (minimum) regret grows logarithmically with arm pulls. Therefore, one can define the limit infimum of regret at time T over log(T) must be more than the informatio ratio i.e. suboptimality gap quotiented (apply moduli space analogy here) out by KL divergence between arms */
/* Dueling bandit regret, cases where a learner can only compare two options and pick one:  define the performance measure to be the infimum of all win probabilities, then the expected dueling bandit regret can be defined as the supremum in expectation of the summation of differences related to this minimum win probability under consideration of this pair of actions. */
/* Safe bandit regret, enforcing a penalty for violating minimum safety standards: take the per round maximum of the forgone reward and the safety violation cost. This structure ensure unsafe arms that are highly rewarding may contributed to high regret. This is also an useful heuristic to consider in real life. */
/* Quantile regret, search space too large, find an arm whose mean reward is above a satisfactory quantile: definition is by benchmarking the expected reward of an arm whose mean reward is the (1-rho)-quantile of all mean rewards. */
/* Pareto regret, regret should measure performance loss relative to tradeoffs: regret is first defined relative to Pareto optimal arms, Pareto regret is then defined to measure the difference between the actual performance and the performance of the optimal arm weighed by a linear combination of objectives */
/* Regret over casual graph: analysis of regret can take place over a graph (directed acyclic, or anything with a poset structure). One define this by considering the pseudoregret. One typically defines this as a complexity measure O(d root T) where d is the number of causal features, T is the number of arm pulls. */
/* Regret proportionality, problem difficulty arises from distinctiveness not arm count: this motivates the approximation of regrets being proportional to the root of the product between the effective dimension (dimension of the action span) and the divergence between distributions (T) */
/* Information ratio, quantifying regret as the price per bit of information: the less information you pull, it is more likely that you will pay in regret. Therefore, one is motivated to define the information ratio as the expectation of r_t^2 (instantaneous regret at time t), note that the t^2 is a normalisation, divided by the mutual information gained at round t. */