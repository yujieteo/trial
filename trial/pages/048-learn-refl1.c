/* Learning, practical: multiple question based adjectives for all concepts with history to be pruned, presented at all layers of abstractions with middle layers presented fast, first principles and big ideas slowed down with ideas leading to rabbit holes explicitly forgotten and added dimensionality when combining ideas in the middle. Present many many abstraction layers.*/
/* Learning and attention, communication between all tokens: ideas used in language may be very far apart. Similarly, for difficult projects, direct communication between people who are distant in ideas and thinking is important.*/
/* Learning and head specialisation: language have different types of relationships like syntax, semantics and so on. A single head produces one representation per token, multiple heads means multiple representations per token or basically for example, an idea as a noun, verb, adjective, reason, so on... An idea from Ravi Vakil, why is an idea; why is a moduli stack? So, try thinking about different representations of an idea.*/
/* Learning, positional encodings and history: it is very easy to mix up the order of tokens or in general historical sequences of events to result in nonsensical arguments. Learn the dates and encode the history of the facts you learn.*/
/* Learning, gradient divergence via repeated multiplication: recall that the chain rule is applied across very deep layers resulting in vanishing or exploding layers. So deriving ideas from proofs are very costly and may blow up. Furthermore, direct representations of simpler ideas can be buried under many layers of understanding and theory (if you try to derive from the basic idea to the big understandings). Therefore, the trick is to really normalise across all layers, or present all layers of abstraction and normalise. Residual connection allows each layer to directly be presented in the output. Need to have learning rate slowly increase. Focus on basic ideas and stick with them for a while until a maximum, then have decay to slow down again at highest level of understanding.*/
/* Learning, masked models: one must anticipate missing (globally or adjacent), or future information. When learning, think about the gaps in your knowledge or how to prepare for what you might know in the future?*/
/* Learning, attention weighting on relevance: focus on the most relevant information in the context of a single attention token.*/
/* Learning, abstraction layers: lower layers have simple patterns, high layers combine them. These are layers of abstraction. */
/* Learning, dropout and overfitting: need to forget some ideas to prevent overfitting to certain phenomena that are oft repeated.*/
/* Learning, sparse attention: each token need not attend to all tokens, but it must be possible to directly communicate with other tokens. That means while you can spend time with adjacent groups of ideas or people, make sure you must have the ability to talk to other groups/ideas/words regardless of how far they are. So the space is as big as possible globally, but sparsity means a lot of effort is spent locally.*/
/* Learning, memory allocation: use less memory for forward and backward pass (aka deriving ideas from layer to layer).*/
/* Learning, label smoothing: never have 100% confidence rate, use a 90% threshold to avoid being stuck (overfitting) and helps with noisy labels.*/
/* Learning, higher dimensionality and multilayer perceptron design: you need more dimensionality to combine features. Start with ideas from two people (say thesis and antithesis), synthesis will require a lot more effort about 4 times as many to capture all the nuances of nonlinearity. */
/* Learning, redundancy and pruning: always consider that unimportant layers, neurons, heads can be pruned and refurbished in cases of limited memory. Example: adjectives for a concept that are not important.s*/